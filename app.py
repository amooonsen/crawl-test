import streamlit as st
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import unicodedata
import io
import csv
from PIL import Image, ImageDraw
import logging
import os
import hashlib
import requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

CACHE_DIR = "screenshot_cache"
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

class SiteIACrawler:
    def __init__(self):
        self.base_url = None
        self.gnb_links = []  # ê³„ì¸µì  êµ¬ì¡°: [{'text': '1ëìŠ¤', 'url': 'url', 'children': [...]}]
        self.side_links = []
        self.footer_links = []
        self.other_links = []

    def setup_driver(self):
        try:
            options = webdriver.ChromeOptions()
            options.add_argument("--headless")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")
            
            # Streamlit Cloud í™˜ê²½ì— ë§ê²Œ ì„¤ì •
            options.binary_location = "/usr/bin/chromium"
            service = Service("/usr/lib/chromium/chromedriver")
            
            driver = webdriver.Chrome(service=service, options=options)
            return driver
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            raise

    def find_gnb_element(self, soup):
        nav_patterns = {
            'semantic_tags': ['nav', 'header'],
            'common_classes': ['gnb', 'nav', 'navigation', 'menu', 'main-menu', 'top-menu', 'topmenu', 'global-nav', 'util-menu', 'user-menu', 'header-menu'],
            'common_ids': ['gnb', 'nav', 'navigation', 'menu', 'top-menu', 'topmenu', 'global-nav', 'util-menu', 'user-menu', 'header-menu']
        }
        potential_navs = []
        for tag in nav_patterns['semantic_tags']:
            elements = soup.find_all(tag)
            potential_navs.extend(elements)
        for class_name in nav_patterns['common_classes']:
            elements = soup.find_all(class_=re.compile(f".*{class_name}.*", re.I))
            potential_navs.extend(elements)
        for id_name in nav_patterns['common_ids']:
            element = soup.find(id=re.compile(f".*{id_name}.*", re.I))
            if element:
                potential_navs.append(element)

        if not potential_navs:
            return None

        scored_elements = []
        for element in potential_navs:
            score = 0
            if element.name == 'nav':
                score += 30
            if element.find_parent('header'):
                score += 20
            links = element.find_all('a')
            score += min(len(links) * 2, 20)
            if element.find(['ul', 'ol']):
                score += 15
            if any('login' in link.get_text().lower() or 'logout' in link.get_text().lower() or 'join' in link.get_text().lower() or 'mypage' in link.get_text().lower() for link in links):
                score += 10  # ë¡œê·¸ì¸, íšŒì›ê°€ì… ë“±ì´ í¬í•¨ëœ ë©”ë‰´ì— ê°€ì‚°ì 
            scored_elements.append((element, score))
        
        return max(scored_elements, key=lambda x: x[1])[0] if scored_elements else None

    def find_side_element(self, soup):
        side_patterns = {
            'semantic_tags': ['aside'],
            'common_classes': ['sidebar', 'side', 'side-menu', 'side-nav'],
            'common_ids': ['sidebar', 'side', 'side-menu']
        }
        potential_sides = []
        for tag in side_patterns['semantic_tags']:
            elements = soup.find_all(tag)
            potential_sides.extend(elements)
        for class_name in side_patterns['common_classes']:
            elements = soup.find_all(class_=re.compile(f".*{class_name}.*", re.I))
            potential_sides.extend(elements)
        for id_name in side_patterns['common_ids']:
            element = soup.find(id=re.compile(f".*{id_name}.*", re.I))
            if element:
                potential_sides.append(element)

        if not potential_sides:
            return None

        scored_elements = []
        for element in potential_sides:
            score = 0
            if element.name == 'aside':
                score += 20
            links = element.find_all('a')
            score += min(len(links) * 2, 20)
            if element.find(['ul', 'ol']):
                score += 15
            scored_elements.append((element, score))
        
        return max(scored_elements, key=lambda x: x[1])[0] if scored_elements else None

    def find_footer_element(self, soup):
        footer_selectors = [
            'footer', '#footer', '.footer',
            '[class*="footer"]', '[class*="Footer"]',
            '.bottom', '#bottom', '.site-bottom'
        ]
        footer_candidates = []
        for selector in footer_selectors:
            elements = soup.select(selector)
            footer_candidates.extend(elements)

        if not footer_candidates:
            all_elements = soup.find_all(['div', 'section'])
            for element in all_elements:
                if any(keyword in element.get_text().lower() for keyword in ['ì´ìš©ì•½ê´€', 'ê°œì¸ì •ë³´', 'ì‚¬ì´íŠ¸ë§µ', 'íšŒì‚¬ì†Œê°œ']):
                    footer_candidates.append(element)

        if not footer_candidates:
            return None

        scored_elements = []
        for element in footer_candidates:
            score = 0
            links = element.find_all('a')
            score += len(links) * 2
            if element.name == 'footer':
                score += 20
            if any(cls and 'footer' in cls.lower() for cls in element.get('class', [])):
                score += 10
            scored_elements.append((element, score))
        
        return max(scored_elements, key=lambda x: x[1])[0] if scored_elements else None

    def extract_links(self, soup, element=None, section="Other", depth=1):
        links = []
        seen = set()

        def process_link(link):
            href = link.get('href', '#')
            text = link.get_text(strip=True)
            if text:
                text = unicodedata.normalize('NFKC', text)
                text = re.sub(r'[\xa0\u200b\u200c\u200d]', '', text)
                text = ' '.join(text.split())
            if not text or href == '#' or href.startswith('javascript:'):
                return None
            url = urljoin(self.base_url, href)
            return (text, url)

        target = element if element else soup
        # ëª¨ë“  <a> íƒœê·¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
        for li in target.find_all('li', recursive=True):  # recursive=Trueë¡œ ëª¨ë“  í•˜ìœ„ ëìŠ¤ íƒìƒ‰
            a_tag = li.find('a')
            if a_tag:
                link_info = process_link(a_tag)
                if link_info and link_info[1] not in seen:  # URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                    seen.add(link_info[1])
                    children = []
                    submenu = li.find(['ul', 'div'], recursive=False)
                    if submenu:
                        for sub_li in submenu.find_all('li', recursive=True):
                            sub_a = sub_li.find('a')
                            if sub_a:
                                sub_link_info = process_link(sub_a)
                                if sub_link_info and sub_link_info[1] not in seen:
                                    seen.add(sub_link_info[1])
                                    children.append({
                                        'text': sub_link_info[0],
                                        'url': sub_link_info[1],
                                        'section': section,
                                        'depth': depth + 1
                                    })
                    links.append({
                        'text': link_info[0],
                        'url': link_info[1],
                        'section': section,
                        'depth': depth,
                        'children': children
                    })
            else:
                submenu = li.find(['ul', 'div'], recursive=False)
                if submenu:
                    links.extend(self.extract_links(soup, submenu, section, depth + 1))

        # <li> êµ¬ì¡°ê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ <a> íƒœê·¸ íƒìƒ‰ (ì˜ˆ: ë¡œê·¸ì¸, íšŒì›ê°€ì… ë²„íŠ¼)
        for a in target.find_all('a', recursive=True):  # recursive=Trueë¡œ ëª¨ë“  í•˜ìœ„ ëìŠ¤ íƒìƒ‰
            link_info = process_link(a)
            if link_info and link_info[1] not in seen:
                seen.add(link_info[1])
                links.append({
                    'text': link_info[0],
                    'url': link_info[1],
                    'section': section,
                    'depth': depth,
                    'children': []
                })
        return links

    def crawl(self, url):
        self.base_url = url
        try:
            # ë¨¼ì € Seleniumìœ¼ë¡œ ì‹œë„
            try:
                driver = self.setup_driver()
                logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
                driver.get(url)
                WebDriverWait(driver, 15).until(
                    lambda d: d.execute_script("return document.readyState") == "complete"
                )

                # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ë™ì  ì½˜í…ì¸  ë¡œë“œ ìœ ë„)
                for _ in range(3):
                    last_height = driver.execute_script("return document.body.scrollHeight")
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1)
                    new_height = driver.execute_script("return document.body.scrollHeight")
                    if new_height == last_height:
                        break

                # ìµœì¢… í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
                soup = BeautifulSoup(driver.page_source, "html.parser")
                driver.quit()
            except Exception as e:
                logger.warning(f"Selenium í¬ë¡¤ë§ ì‹¤íŒ¨, requestsë¡œ ëŒ€ì²´: {str(e)}")
                # Selenium ì‹¤íŒ¨ ì‹œ requests ì‚¬ìš©
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
                }
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")

            # GNB ë° Top Menu ì¶”ì¶œ
            gnb_element = self.find_gnb_element(soup)
            self.gnb_links = self.extract_links(soup, gnb_element, "GNB", depth=1) if gnb_element else []
            self.side_links = self.extract_links(soup, self.find_side_element(soup), "Side Menu") if self.find_side_element(soup) else []
            self.footer_links = self.extract_links(soup, self.find_footer_element(soup), "Footer") if self.find_footer_element(soup) else []

            all_links = self.extract_links(soup, section="Other")
            seen_urls = set()
            for link in self.gnb_links + self.side_links + self.footer_links:
                seen_urls.add(link['url'])
                for child in link.get('children', []):
                    seen_urls.add(child['url'])
            self.other_links = [link for link in all_links if link['url'] not in seen_urls]

            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {url}")
            return True
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return str(e)

    def generate_txt(self):
        output = io.StringIO()
        output.write(f"ì‚¬ì´íŠ¸ IA êµ¬ì¡°ë„ ({self.base_url})\n")
        output.write("=" * 50 + "\n\n")

        output.write("ğŸ“Œ GNB ë©”ë‰´\n")
        if self.gnb_links:
            for link in self.gnb_links:
                output.write(f"â”œâ”€â”€ {link['text']} - {link['url']}\n")
                for child in link.get('children', []):
                    output.write(f"â”‚   â”œâ”€â”€ {child['text']} - {child['url']}\n")
        else:
            output.write("â”œâ”€â”€ GNB ë°ì´í„° ì—†ìŒ\n")
        output.write("\n")

        output.write("ğŸ“Œ Side Menu\n")
        if self.side_links:
            for link in self.side_links:
                output.write(f"â”œâ”€â”€ {link['text']} - {link['url']}\n")
                for child in link.get('children', []):
                    output.write(f"â”‚   â”œâ”€â”€ {child['text']} - {child['url']}\n")
        else:
            output.write("â”œâ”€â”€ Side Menu ë°ì´í„° ì—†ìŒ\n")
        output.write("\n")

        output.write("ğŸ“Œ Footer ë©”ë‰´\n")
        if self.footer_links:
            for link in self.footer_links:
                output.write(f"â”œâ”€â”€ {link['text']} - {link['url']}\n")
                for child in link.get('children', []):
                    output.write(f"â”‚   â”œâ”€â”€ {child['text']} - {child['url']}\n")
        else:
            output.write("â”œâ”€â”€ Footer ë°ì´í„° ì—†ìŒ\n")
        output.write("\n")

        output.write("ğŸ“Œ ê¸°íƒ€ ë§í¬\n")
        if self.other_links:
            for link in self.other_links:
                output.write(f"â”œâ”€â”€ {link['text']} - {link['url']}\n")
                for child in link.get('children', []):
                    output.write(f"â”‚   â”œâ”€â”€ {child['text']} - {child['url']}\n")
        else:
            output.write("â”œâ”€â”€ ê¸°íƒ€ ë°ì´í„° ì—†ìŒ\n")

        return output.getvalue()

    def generate_csv(self):
        output = io.StringIO()
        output.write('\ufeff')
        writer = csv.writer(output, lineterminator='\n')
        writer.writerow(['ì„¹ì…˜', 'í…ìŠ¤íŠ¸', 'URL', 'ëìŠ¤'])

        def write_links(links, section):
            for link in links:
                writer.writerow([section, link['text'], link['url'], link['depth']])
                for child in link.get('children', []):
                    writer.writerow([section, child['text'], child['url'], child['depth']])

        write_links(self.gnb_links, 'GNB')
        write_links(self.side_links, 'Side Menu')
        write_links(self.footer_links, 'Footer')
        write_links(self.other_links, 'Other')

        return output.getvalue()

    def generate_md(self):
        output = io.StringIO()
        output.write(f"# ì‚¬ì´íŠ¸ IA êµ¬ì¡°ë„ ({self.base_url})\n\n")

        output.write("## ğŸ“Œ GNB ë©”ë‰´\n\n")
        if self.gnb_links:
            for link in self.gnb_links:
                output.write(f"- [{link['text']}]({link['url']})\n")
                for child in link.get('children', []):
                    output.write(f"  - [{child['text']}]({child['url']})\n")
        else:
            output.write("GNB ë°ì´í„° ì—†ìŒ\n\n")

        output.write("\n## ğŸ“Œ Side Menu\n\n")
        if self.side_links:
            for link in self.side_links:
                output.write(f"- [{link['text']}]({link['url']})\n")
                for child in link.get('children', []):
                    output.write(f"  - [{child['text']}]({child['url']})\n")
        else:
            output.write("Side Menu ë°ì´í„° ì—†ìŒ\n\n")

        output.write("\n## ğŸ“Œ Footer ë©”ë‰´\n\n")
        if self.footer_links:
            for link in self.footer_links:
                output.write(f"- [{link['text']}]({link['url']})\n")
                for child in link.get('children', []):
                    output.write(f"  - [{child['text']}]({child['url']})\n")
        else:
            output.write("Footer ë°ì´í„° ì—†ìŒ\n\n")

        output.write("\n## ğŸ“Œ ê¸°íƒ€ ë§í¬\n\n")
        if self.other_links:
            for link in self.other_links:
                output.write(f"- [{link['text']}]({link['url']})\n")
                for child in link.get('children', []):
                    output.write(f"  - [{child['text']}]({child['url']})\n")
        else:
            output.write("ê¸°íƒ€ ë°ì´í„° ì—†ìŒ\n")

        return output.getvalue()

    def get_cache_path(self, url, width):
        hash_key = hashlib.md5(f"{url}_{width}".encode()).hexdigest()
        return os.path.join(CACHE_DIR, f"{hash_key}.png")

    def handle_popup(self, driver):
        try:
            close_button_selectors = [
                'button.close',
                'a.close',
                '[class*="close"]',
                '[id*="close"]',
                'button[aria-label="Close"]',
                'button[aria-label="close"]',
                'div.close-btn',
                '.btn-close',
                '.modal-close'
            ]

            for selector in close_button_selectors:
                try:
                    close_button = WebDriverWait(driver, 3).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                    )
                    close_button.click()
                    logger.info("íŒì—… ë‹«ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                    time.sleep(1)
                    return True
                except Exception as e:
                    continue

            logger.info("ë‹«ê¸° ë²„íŠ¼ì„ ì°¾ì§€ ëª»í•´ íŒì—… ìš”ì†Œ ìˆ¨ê¹€ ì²˜ë¦¬")
            driver.execute_script("""
                var popups = document.querySelectorAll('.modal, .popup, .overlay, [id*="popup"], [class*="popup"], [id*="modal"], [class*="modal"], [id*="overlay"], [class*="overlay"]');
                popups.forEach(function(el) {
                    el.style.display = 'none';
                });
            """)
            return True
        except Exception as e:
            logger.warning(f"íŒì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    def capture_screenshot(self, url, width):
        cache_path = self.get_cache_path(url, width)
        
        if os.path.exists(cache_path):
            logger.info(f"ìºì‹œì—ì„œ ìŠ¤í¬ë¦°ìƒ· ë¡œë“œ: {url} (width: {width})")
            with open(cache_path, 'rb') as f:
                return f.read()

        try:
            driver = self.setup_driver()
            try:
                logger.info(f"ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹œì‘: {url} (width: {width})")
                driver.set_window_size(width, 1080)
                driver.get(url)

                WebDriverWait(driver, 10).until(
                    lambda d: d.execute_script("return document.readyState") == "complete"
                )

                self.handle_popup(driver)

                driver.execute_script("document.body.style.overflow = 'hidden';")

                for _ in range(3):
                    last_height = driver.execute_script("return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);")
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1)
                    new_height = driver.execute_script("return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);")
                    if new_height == last_height:
                        break

                driver.set_window_size(width, last_height)
                time.sleep(1)

                screenshot = driver.get_screenshot_as_png()
                logger.info(f"ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì™„ë£Œ: {url}")

                with open(cache_path, 'wb') as f:
                    f.write(screenshot)

                return screenshot
            finally:
                driver.quit()
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {url} - {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ì´ë¯¸ì§€ ìƒì„±
            img = Image.new('RGB', (width, 400), color = (240, 240, 240))
            d = ImageDraw.Draw(img)
            d.text((20, 20), f"ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {url}", fill=(0, 0, 0))
            d.text((20, 50), "Streamlit Cloud í™˜ê²½ì—ì„œ ìŠ¤í¬ë¦°ìƒ· ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.", fill=(0, 0, 0))
            img_bytes = io.BytesIO()
            img.save(img_bytes, format='PNG')
            img_bytes.seek(0)
            
            # ìºì‹œì— ì €ì¥
            with open(cache_path, 'wb') as f:
                f.write(img_bytes.getvalue())
                
            return img_bytes.getvalue()

# Streamlit UI
st.set_page_config(page_title="ì‚¬ì´íŠ¸ IA êµ¬ì¡°ë„ í¬ë¡¤ëŸ¬", layout="wide")
st.title("ì‚¬ì´íŠ¸ IA êµ¬ì¡°ë„ í¬ë¡¤ëŸ¬")

url = st.text_input("í¬ë¡¤ë§í•  URLì„ ì…ë ¥í•˜ì„¸ìš”")

if st.button("í¬ë¡¤ë§ ì‹œì‘"):
    if not url:
        st.error("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        if not urlparse(url).scheme:
            url = "https://" + url
        
        with st.spinner("í¬ë¡¤ë§ ì¤‘..."):
            crawler = SiteIACrawler()
            result = crawler.crawl(url)
            
            if result is True:
                st.success("í¬ë¡¤ë§ ì™„ë£Œ!")
                
                # GNB ë©”ë‰´ í‘œì‹œ
                st.header("ğŸ“Œ GNB ë©”ë‰´")
                if crawler.gnb_links:
                    for link in crawler.gnb_links:
                        st.markdown(f"- [{link['text']}]({link['url']})")
                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button(f"PC ìŠ¤í¬ë¦°ìƒ· - {link['text']}", key=f"pc_{link['url']}"):
                                screenshot = crawler.capture_screenshot(link['url'], 1920)
                                st.image(screenshot)
                        with col2:
                            if st.button(f"ëª¨ë°”ì¼ ìŠ¤í¬ë¦°ìƒ· - {link['text']}", key=f"mo_{link['url']}"):
                                screenshot = crawler.capture_screenshot(link['url'], 360)
                                st.image(screenshot)
                        
                        if link.get('children'):
                            for child in link['children']:
                                st.markdown(f"  - [{child['text']}]({child['url']})")
                else:
                    st.info("GNB ë°ì´í„° ì—†ìŒ")
                
                # Side Menu í‘œì‹œ
                st.header("ğŸ“Œ Side Menu")
                if crawler.side_links:
                    for link in crawler.side_links:
                        st.markdown(f"- [{link['text']}]({link['url']})")
                        if link.get('children'):
                            for child in link['children']:
                                st.markdown(f"  - [{child['text']}]({child['url']})")
                else:
                    st.info("Side Menu ë°ì´í„° ì—†ìŒ")
                
                # Footer ë©”ë‰´ í‘œì‹œ
                st.header("ğŸ“Œ Footer ë©”ë‰´")
                if crawler.footer_links:
                    for link in crawler.footer_links:
                        st.markdown(f"- [{link['text']}]({link['url']})")
                        if link.get('children'):
                            for child in link['children']:
                                st.markdown(f"  - [{child['text']}]({child['url']})")
                else:
                    st.info("Footer ë°ì´í„° ì—†ìŒ")
                
                # ê¸°íƒ€ ë§í¬ í‘œì‹œ
                st.header("ğŸ“Œ ê¸°íƒ€ ë§í¬")
                if crawler.other_links:
                    for link in crawler.other_links:
                        st.markdown(f"- [{link['text']}]({link['url']})")
                        if link.get('children'):
                            for child in link['children']:
                                st.markdown(f"  - [{child['text']}]({child['url']})")
                else:
                    st.info("ê¸°íƒ€ ë°ì´í„° ì—†ìŒ")
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                col1, col2, col3 = st.columns(3)
                with col1:
                    txt_content = crawler.generate_txt()
                    st.download_button(
                        label="TXT ë‹¤ìš´ë¡œë“œ",
                        data=txt_content,
                        file_name="site_ia.txt",
                        mime="text/plain"
                    )
                with col2:
                    csv_content = crawler.generate_csv()
                    st.download_button(
                        label="CSV ë‹¤ìš´ë¡œë“œ",
                        data=csv_content,
                        file_name="site_ia.csv",
                        mime="text/csv"
                    )
                with col3:
                    md_content = crawler.generate_md()
                    st.download_button(
                        label="MD ë‹¤ìš´ë¡œë“œ",
                        data=md_content,
                        file_name="site_ia.md",
                        mime="text/markdown"
                    )
            else:
                st.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {result}")